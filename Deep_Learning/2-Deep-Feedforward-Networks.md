---
title: 2 Deep Feedforward Networks
date: 2018-09-17 19:29:23
category: Deep_Learning
---
<font size=6>深度前馈网络
<!--more-->

---
<font size=5>深度前馈网络
<font size=3>深度前馈网络也叫前馈神经网络或多层感知机，是最典型的深度学习模型。深度前馈网络的目标就是拟合一个函数。举个例子，有一个分类器$y=f^*(x)$将输入$x$映射到输出类别$y$。深度前馈网将这个映射定义为$f(x,\theta)$并学习这个参数$\theta$的值来得到最好的函数拟合。

深度前馈网络的前馈代表着信息从$x$流入，通过中间$f$的计算，最后到达输出$y$。而且在输出和模型自身之间没有反馈连接。深度前馈网络是许多商业应用的基础，比如，用来图像识别的卷积神经网络就是一种前向反馈网络，用于自然语言应用中的递归神经网络的基石也是前向反馈网络。深度前馈网络之所以被称之为网络，是因为这个模型可以使用一张有向无环图来描述函数是如何复合的。假设我们有$f^{(1)},f^{(2)},f^{(3)}$这三个函数链式连接，这个链式连接可以表示为$f(x)=f^{(3)}(f^{(2)}(f^{(1)}(x)))$,这种链式结构就是神经网络最为常用的结构。$f^{(1)},f^{(2)},f^{(3)}$被称为神经网络的第一层，第二层，第三层，深度前馈网络最后一层就是输出层。这个链的长度就是神经网络的深度。

训练数据为我们提供了在不同训练点上取值的、含有噪声的$f^*(x)$的近似实例。训练样本直接指明了输出层在每一点$x$上的输出值一定要接近该点的$y$值。但是训练样本并没有直接指明其他层的行为，而是学习算法应该决定这些层如何输出期望值。因为训练数据并没有给出这些层中的每一层所需的输出，所以这些层被称为隐藏层。每个隐层可以认为是由一些并行操作的单元组成，或者我们可以将这些单元看作为神经元。我们可以使用向量来对网络中的隐层进行表示。隐层的神经元数或者向量维数就是神经网络的宽度。

深度前馈网络的目标并不是为了对大脑进行建模，而是为了实现统计泛化二设计出的函数拟合机器。虽然神经网络或多或少受到了神经科学的影响，它从大脑的工作方式得到了一些灵感，但是现在神经网络的研究其实受到数学和工程的指引会更多一些。

为了对深度前馈网络有一个更加深刻地理解，我们先从逻辑回归和线性回归这些线性模型开始。线性模型无论是通过闭解形式还是使用凸优化，它们都能高效且可靠地拟合数据。但是线性模型无法克服的缺点就是线性模型的能力被限制在线性函数中，所以它无法理解任何两个输入变量间的相互作用。为了扩展线性模型来表示$x$的非线性函数，我们不再将$x$作为输入，而是使用其非线性变换后的$\phi (x)$作为输入。同理，使用支持向量机中核方法也来得到一个非线性映射。另一个问题随之而来，我们该选择怎样的$\phi$呢?接下来讨论一些选择 : 
- 其中一种选择是使用一个通用的$\phi$，例如无限维的$\phi$，它隐含地用在基于RBF核的核机器上。如果$\phi (x)$具有足够高的维数，我们总是有足够的能力来拟合训练集，但是对于测试集的泛化往往不佳。非常通用的特征映射通常只基于局部光滑的原则，并且没有将足够的先验信息进行编码来解决高级问题。
- 另一种选择是手动地设计$\phi$。在深度学习出现以前，这一直是主流的方法。这种方法对于每个单独的任务都需要人们数十年的努力，从业者各自擅长特定的领域，并且不同领域之间很难迁移。
- 深度学习的策略是去学习$\phi$。在这种方法中，我们有一个模型$y=\phi (x,\theta))^T \omega$。我们现在有两种参数：用于从一大类函数中学习$\phi$的参数$\theta$，以及用于将$\phi (x)$映射到所需的输出的参数$\omega$。这是深度前馈网络的一个例子，其中$\phi$定义了一个隐藏层。这是三种方法中唯一一种放弃了训练问题的凸性的，但是利大于弊。在这种方法中，我们将表示参数化为$\phi (x,\theta)$,并且使用优化算法来寻找$\theta$，使它能够得到一个好的表示。如果我们想要的话，这种方法也可以通过使它变得高度通用以获得第一种方法的优点——我们只需使用一个非常广泛的函数族$\phi (x,\theta)$。这种方法也可以获得第二种方法的优点。人类专家可以将他们的知识编码进网络来帮助泛化，他们只需要设计那些他们期望能够表现优异的函数族$\phi (x,\theta)$即可。这种方法的优点是人类设计者只需要寻找正确的函数族即可，而不需要去寻找精确的函数。
