---
title: 2 Deep Feedforward Networks
date: 2018-09-17 19:29:23
category: Deep_Learning
---
<font size=6>深度前馈网络
<!--more-->

---
<font size=5>深度前馈网络
<font size=3>深度前馈网络也叫前馈神经网络或多层感知机，是最典型的深度学习模型。深度前馈网络的目标就是拟合一个函数。举个例子，有一个分类器$y=f^*(x)$将输入$x$映射到输出类别$y$。深度前馈网将这个映射定义为$f(x,\theta)$并学习这个参数$\theta$的值来得到最好的函数拟合。

深度前馈网络的前馈代表着信息从$x$流入，通过中间$f$的计算，最后到达输出$y$。而且在输出和模型自身之间没有反馈连接。深度前馈网络是许多商业应用的基础，比如，用来图像识别的卷积神经网络就是一种前向反馈网络，用于自然语言应用中的递归神经网络的基石也是前向反馈网络。深度前馈网络之所以被称之为网络，是因为这个模型可以使用一张有向无环图来描述函数是如何复合的。假设我们有$f^{(1)},f^{(2)},f^{(3)}$这三个函数链式连接，这个链式连接可以表示为$f(x)=f^{(3)}(f^{(2)}(f^{(1)}(x)))$,这种链式结构就是神经网络最为常用的结构。$f^{(1)},f^{(2)},f^{(3)}$被称为神经网络的第一层，第二层，第三层，深度前馈网络最后一层就是输出层。这个链的长度就是神经网络的深度。

训练数据为我们提供了在不同训练点上取值的、含有噪声的$f^*(x)$的近似实例。训练样本直接指明了输出层在每一点$x$上的输出值一定要接近该点的$y$值。但是训练样本并没有直接指明其他层的行为，而是学习算法应该决定这些层如何输出期望值。因为训练数据并没有给出这些层中的每一层所需的输出，所以这些层被称为隐藏层。每个隐层可以认为是由一些并行操作的单元组成，或者我们可以将这些单元看作为神经元。我们可以使用向量来对网络中的隐层进行表示。隐层的神经元数或者向量维数就是神经网络的宽度。

深度前馈网络的目标并不是为了对大脑进行建模，而是为了实现统计泛化二设计出的函数拟合机器。虽然神经网络或多或少受到了神经科学的影响，它从大脑的工作方式得到了一些灵感，但是现在神经网络的研究其实受到数学和工程的指引会更多一些。

为了对深度前馈网络有一个更加深刻地理解，我们先从逻辑回归和线性回归这些线性模型开始。线性模型无论是通过闭解形式还是使用凸优化，它们都能高效且可靠地拟合数据。但是线性模型无法克服的缺点就是线性模型的能力被限制在线性函数中，所以它无法理解任何两个输入变量间的相互作用。为了扩展线性模型来表示$x$的非线性函数，我们不再将$x$作为输入，而是使用其非线性变换后的$\phi (x)$作为输入。同理，使用支持向量机中核方法也来得到一个非线性映射。另一个问题随之而来，我们该选择怎样的$\phi$呢?接下来讨论一些选择 : 
- 其中一种选择是使用一个通用的$\phi$，例如隐含地用在基于RBF核的核机器上的无限维的$\phi$。如果$\phi (x)$具有足够高的维数，我们总是有足够的能力来拟合训练集，但是这样训练出来的模型容易造成过拟合而造成对于测试集的泛化往往不佳。非常通用的$\phi$通常只基于局部光滑的原则，并且没有将足够的先验信息进行编码来解决高级问题。
- 另一种选择是手动地设计$\phi$。在深度学习出现以前，这一直是主流的方法。这种方法对于每个单独的任务都需要擅长一些特定的领域人们数十年的努力，而且不同领域的任务之间很难进行迁移。
- 深度学习的策略是去学习$\phi$。在这种方法中，我们有一个模型$y=\phi (x,\theta)^T \omega$。我们现在有两种参数：用于从一大类函数中学习$\phi$的参数$\theta$，以及用于将$\phi (x)$映射到所需的输出的参数$\omega$。这是深度前馈网络的一个例子，其中$\phi$定义了一个隐藏层。这是三种方法中唯一一种放弃了训练问题的凸性的，但是利大于弊。在这种方法中，我们将表示参数化为$\phi (x,\theta)$,并且使用优化算法来寻找$\theta$，使它能够得到一个好的表示。这种方法也可以通过使它变得高度通用以获得第一种方法的优点，我们只需使用一个非常广泛的函数族$\phi (x,\theta)$。这种方法也可以获得第二种方法的优点。人类专家可以将他们的知识编码进网络来帮助泛化，他们只需要设计那些他们期望能够表现优异的函数族$\phi (x,\theta)$即可。这种方法的优点是人类设计者只需要寻找正确的函数族即可，而不需要去寻找精确的函数。
<br/>

<font size=5>深度学习例子 : 学习异或
<font size=3>为了让深度前馈网络更加具体一些，我们可以实现一个非常简单的任务——异或。我们假设异或的目标函数是$y=f^*(x)$。我们的模型使用函数$y=f(x,\theta)$来进行拟合。

因为两个数的异或的数据集就只有4个样本
$$\mathbb{X}=\{(0,0)^T,(1,0)^T,(0,1)^T,(1,1)^T\}$$
所以在这里我们不考虑统计泛化，只需拟合训练集即可。同时我们使用平方差
$$J(\theta)=\frac{1}{4}\sum_{x \in \mathbb{X}}(f^*(x)-f(x,\theta))^2$$
来作为损失函数，虽然平方差作为损失函数对于离散数据并不是一个好的选择。合适的方法将会在后面进行介绍。

接下来开始选择模型.我们先选择线性模型，因为通常情况下，线性模型也具有不错的性能。$\theta$包含了$\omega$和$b$。这个模型被定义为 : 
$$f(x,\theta)=f(x,\omega,b)=\omega^Tx+b$$
使用正规方程来最小化损失函数得到的闭式解为$\omega=0,b=\frac{1}{2}$，这个线性模型对于所有输入都只能输出$\frac{1}{2}$。这个结果显然是不对的，下图解释了为什么线性模型不能表示异或 : 
![Deep_Learning_Figure_6.1](https://winteryangwt-1256492362.cos.ap-chengdu.myqcloud.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Deep_Learning_Figure_6.1.png)
当$x_1=0$，输出随着$x_2$的增大而增大，但是当$x_1=1$,输出随着$x_2$的增大而减小。换言之我们不能通过一条线来将输出一样的样本划开或者不是线性可分的。

为了能够表示异或，我们在原来的模型中加入一个隐层，这个隐层有两个单元。现在这个模型的形式为
$$f(x,\theta_1,\theta_2)=f_2(f_1(x,\omega_1,b_1),\omega_2,b_2)$$
我们应注意到$f_1$不能为线性函数，因为当$f_1$，$f_2$都为线性函数时，假设$f_1=\omega_1^Tx,f_2=\omega_2^Tf_1$,会得到$f=\omega_2^T\omega_1^Tx=\omega^Tx(\omega^T=\omega_2^T\omega_1^T)$，这个模型和原来的模型一样都是线性模型。所以我们必须使用非线性函数来描述这些特征。大多数神经网络通过在使用参数进行仿射变换之后紧跟着一个被称为激活函数的固定非线性函数来实现这个目标,表示为$f_1=g(\omega_1^Tx+b_1)$。

在使用上面的模型后可以得到一个解
$$
 \omega_1=\left[
 \begin{matrix}
   1 & 1 \\
   1 & 1 \\
\end{matrix}
\right]
$$
$$b_1=\left[
\begin{matrix}
0 \\
1 \\
\end{matrix}
\right]
$$
$$\omega_2=\left[
\begin{matrix}
1 & -2
\end{matrix}
\right]
$$
$$x=\left[
\begin{matrix}
0 & 0 & 1 & 1 \\
0 & 1 & 0 & 1 \\
\end{matrix}
\right]
$$
我们默认使用整流线性单元Relu函数
$$g(z)=max(0,z)$$
来作为固定非线性函数。
$$f(x,\theta_1,\theta_2)=f_2(f_1(x,\omega_1,b_1),\omega_2,b_2)=\omega_2^Tg(\omega_1^Tx+b_1)=\left[
\begin{matrix}
0 & 1 & 1 & 0
\end{matrix}
\right]$$
可以看出，新的模型对于$\mathbb{X}$中的样本都给出了正确的结果。这个例子非常简单，但是在实际情况中,可能会有数十亿的模型参数以及数十亿的训练样本，所以不能像我们这里做的那样进行简单地猜解。所以可以使用基于梯度的优化算法来找到使误差非常小的参数。
<br/>
